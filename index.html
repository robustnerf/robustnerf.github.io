<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>RobustNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://d2nerf.github.io/img/title_card.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://github.com/d2nerf/d2nerf">
    <meta property="og:title" content="D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video">
    <meta property="og:description" content="Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video">
    <meta name="twitter:description" content="Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.">
    <meta name="twitter:image" content="https://d2nerf.github.io/img/title_card.png"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>RobustNeRF</b>: Ignoring Distractors with Robust Losses<br>
                <small>
                    CVPR 2023 (highlight paper)
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com/citations?user=l8wQ39EAAAAJ&hl=en">
                              Sara Sabour<sup>1,2</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com/citations?user=mOOsGrQAAAAJ&hl=en">
                              Suhani Vora<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.stronglyconvex.com/about.html">
                             Daniel Duckworth<sup>1</sup>
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://www.catalyzex.com/author/Ivan%20Krasin">
                                Ivan Krasin<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cs.toronto.edu/~fleet/">
                              David J. Fleet<sup>1,2</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://taiya.github.io/">
                              Andrea Tagliasacchi<sup>1,2,3</sup>
                            </a>
                        </td>
                    </tr>
                </table>                
            </div>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <p class="text-center">
                        Google Deepmind<sup>1</sup>&emsp; University of Toronto<sup>2</sup>&emsp; Simon Fraser University<sup>3</sup>
                    </p>
                </div>
            </div>    
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-md-4 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2302.00833">
                            <img src="./img/paper_icon.png" height="90px">
                                <h4><strong>Paper (arxiv)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/vCeRgc1lahU">
                            <img src="./img/youtube_icon.png" height="90px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://storage.googleapis.com/jax3d-public/projects/robustnerf/robustnerf.tar.gz">
                            <img src="./img/data_icon.webp" height="90px">
                                <h4><strong>Data</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://drive.google.com/file/d/146INnaOtSJNCKWWcJS4YqFOYGCxCn_1k/view?usp=sharing">
                            <img src="./img/slides_icon.png" height="90px">
                                <h4><strong>Slides</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://drive.google.com/file/d/1Ri3unWyr1-5ttsIua68A2nJSfpSrKDtW/view?usp=sharing">
                            <img src="./img/poster_icon.png" height="90px">
                                <h4><strong>Poster</strong></h4>
                            </a>
                        </li>                        
                        <!-- <li>
                            <a href="https://github.com/d2nerf/d2nerf">
                            <img src="./img/github_icon.svg" height="90px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>

        

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="balloonDiv">
                    <video class="video" id="balloon" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="balloonMerge"></canvas>
                </div>
			</div>
        </div> -->



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Teaser
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <video class="video" preload="auto" style="position:absolute;top:0;left:0;width:100%;height:100%;" loop autoPlay muted src="video/robustnerf_twitter.m4v"></video>
                    </div>
                </div>
            </div>
        </div>
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene. When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as  view-dependent effects or 'floaters'. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distractors in training data as outliers of an optimization problem. Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern NeRF frameworks, with 
                    few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects.
                </p>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                Comparisons of RobustNeRF(left) and MipNeRF360 (right) on BabyYoda Distractors Dataset.

                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water4" loop playsinline autoPlay muted src="video/yoda_robustnerf_split.mp4" onplay="resizeAndPlayDualWhenReady(this, 'water5')"></video>
                                
                                <canvas height=0 class="videoMerge" id="water4Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water5" loop playsinline autoPlay muted src="video/yoda_360_split.mp4"></video>
                                
                                <canvas height=0 class="videoMerge" id="water5Merge"></canvas>
                        </td>
                    </tr>
                </table>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            </br>Comparisons of RobustNeRF(left) and MipNeRF360 (right) on Crab Distractors Dataset.</br>

                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water2" loop playsinline autoPlay muted src="video/robustnerf_crab_split.mp4" onplay="resizeAndPlayDualWhenReady(this, 'water3')"></video>
                                
                                <canvas height=0 class="videoMerge" id="water2Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water3" loop playsinline autoPlay muted src="video/360_crab_split.mp4"></video>
                                
                                <canvas height=0 class="videoMerge" id="water3Merge"></canvas>
                        </td>
                    </tr>
                </table>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            </br>Comparisons of RobustNeRF(left) and MipNeRF360 (right) on Statue Distractors Dataset.</br>

                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water6" loop playsinline autoPlay muted src="video/t_balloon_robustnerf_split.mp4" onplay="resizeAndPlayDualWhenReady(this, 'water7')"></video>
                                
                                <canvas height=0 class="videoMerge" id="water6Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water7" loop playsinline autoPlay muted src="video/t_balloon_360_split.mp4"></video>
                                
                                <canvas height=0 class="videoMerge" id="water7Merge"></canvas>
                        </td>
                    </tr>
                </table>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            </br>Comparisons of RobustNeRF(left) and MipNeRF360 (right) on Android Distractors Dataset.</br>

                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water8" loop playsinline autoPlay muted src="video/robustnerf_and-bot_split.mp4" onplay="resizeAndPlayDualWhenReady(this, 'water9')"></video>
                                
                                <canvas height=0 class="videoMerge" id="water8Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water9" loop playsinline autoPlay muted src="video/360_and-bot_split.mp4"></video>
                                
                                <canvas height=0 class="videoMerge" id="water9Merge"></canvas>
                        </td>
                    </tr>
                </table>

            </div>
        </div>
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="waterDiv">
                    <video class="video" id="water" loop playsinline autoPlay muted src="video/vrig_vrig_water_pour.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="waterMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">
                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/vrig_vrig_duck_jump_compare_test.mp4" type="video/mp4" />
                    </video>
                </div>
                Please refresh the page if videos are not loaded or appear unsynced.
            </div>
        </div> -->


        <!-- <image src="img/method.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">




            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        More results
                    </h3>

                    <div class="text-justify">
                        Our method decouples and reconstructs 3D models of dynamic occluders and static background from a monocular video:
                        <br><br>
                        
                    </div>
                    
                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="balloon1" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop.mp4" onplay="resizeAndPlayDual(this, 'balloon2')"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="balloon1Merge"></canvas>
                            </td>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="balloon2" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop_compare_dynamic.mp4"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="balloon2Merge"></canvas>
                            </td>
                        </tr>
                    </table>
    
                </div>
            </div>

            <div class="row" height="400">
                <div class="col-md-8 col-md-offset-2">
                    <div>
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                            <source src="video/vrig_balloon_wave_crop_compare_test.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">

                <div class="col-md-8 col-md-offset-2">

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark1" loop playsinline autoPlay muted src="video/shark_scam.mp4" onplay="resizeAndPlayDual(this, 'shark2')"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="shark1Merge"></canvas>
                            </td>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark2" loop playsinline autoPlay muted src="video/shark_scam_compare_dynamic.mp4"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="shark2Merge"></canvas>
                            </td>
                        </tr>
                    </table>

                </div>
            </div>
        </div> -->




        <!-- <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">

                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div> -->


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    Our method can also remove challenging view-correlated shadows, such as shadows cast by the camera or the photographer:
                    <br><br>
                    
                </div>
                <div class="video-compare-container" id="camShadowDiv">
                    <video class="video" id="camShadow" loop playsinline autoPlay muted src="video/camera_shadow_v2.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="camShadowMerge"></canvas>
                </div>
			</div>
        </div> -->

<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    <br><br>
                    Our method learns decoupled neural radiance fields, we can therefore render separate components from novel view. As a by-product, we better utilize the network capacity and learn more robust dynamic objects in the scene:  
                    <br><br>
                </div>
                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/hypernerf_broom-vcam_compare_nv.mp4" type="video/mp4" />
                    </video>
                </div>
                <div class="video-compare-container-small" id="nvDiv">
                    <video class="video" id="nv" loop playsinline autoPlay muted src="video/hypernerf_broom-vcam_compare_nv_hn.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="nvMerge"></canvas>
                </div>
			</div>
        </div> -->


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reflection Direction Parameterization
                </h3>
                <div class="text-justify">
                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.
                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Directional Encoding
                </h3>
                <div class="text-justify">
                    We explicitly model object roughness using the expected values of a set of spherical harmonics under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows sharing the emittance functions between points with different roughnesses. It also enables scene editing after training. Theoretically, our encoding is stationary on the sphere, similar to the Euclidean stationarity of NeRF's positional encoding. <br><br>
                </div>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Synthetic Results
                </h3>
                <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div>
			</div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Captured Scenes
                </h3>
                Our method also produces accurate renderings and surface normals from captured photographs:
                <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>
                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted>
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>
                
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>
                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>
                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div> -->

            
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{verbin2021refnerf,
    title={{Ref-NeRF}: Structured View-Dependent Appearance for
           Neural Radiance Fields},
    author={Dor Verbin and Peter Hedman and Ben Mildenhall and
            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
    journal={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Datasets Release
                </h3>
                <p class="text-justify">
                ETA July 30th. Please reach out to sasasabour@google with enquiry in meantime!
                </p>
            </div>
        </div> -->
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
